{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinViz top gainer package preparations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19s8s2X301K779atIqxd5FUkBJ4xNeG75",
      "authorship_tag": "ABX9TyNiyvdsFo2SHUsDN6DV6uvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrunner21/Trading/blob/main/FinViz_top_gainer_package_preparations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3paBX-Nuf1u"
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "import datetime\n",
        "from datetime import date"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xi4lt7nvUZa"
      },
      "source": [
        "def page_amounts(url):\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "  page_count = html.find_all(attrs = {'class': 'screener-pages'})\n",
        "\n",
        "  p_count = 1\n",
        "  for i in page_count:\n",
        "    p_count += 1\n",
        "  return p_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p-pWwiYqJxg"
      },
      "source": [
        "def page_read(url):\n",
        "  top_gainers = []\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "  for ticker in gainers:\n",
        "    top_gainers.append(ticker.get_text().strip())\n",
        "  return top_gainers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3B2kF_rSZd"
      },
      "source": [
        "def multiple_pages(page_url, page_number):\n",
        "  top_gainers = []\n",
        "  pages = np.arange(21, ((page_number * 20) + 1), 20)\n",
        "  for page in pages:\n",
        "    url = (page_url +\"&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    for ticker in gainers:\n",
        "      top_gainers.append(ticker.get_text().strip())\n",
        "    sleep(2) #so we can get all pages every single time\n",
        "  return top_gainers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyFuxi3Bm4PI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c49314b-a88c-4b0f-ddf5-3f0e3d109b5e"
      },
      "source": [
        "utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
        "today = utc_now.astimezone(pytz.timezone(\"America/Chicago\"))\n",
        "new_today = today.strftime(\"%m-%d-%Y\")\n",
        "filename = 'top_gainers_' + new_today + '.csv'\n",
        "\n",
        "top_gainers = []\n",
        "\n",
        "def get_top_gainers2(url, top_gainers):\n",
        "  p_count = page_amounts(url)\n",
        "  top_gainers = page_read(url)\n",
        "  top_gainers = top_gainers + multiple_pages(url, p_count)\n",
        "  return top_gainers\n",
        "\n",
        "# df = pd.DataFrame( get_top_gainers2(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers\", top_gainers) )\n",
        "# df.to_csv(\"drive/My Drive/stocks daily top gainers/\" + filename)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0\n",
            "0    FLGC\n",
            "1    GOVX\n",
            "2    PXLW\n",
            "3    VIEW\n",
            "4    AXDX\n",
            "..    ...\n",
            "195  PPSI\n",
            "196    SI\n",
            "197  AGMH\n",
            "198  VTEX\n",
            "199   WOW\n",
            "\n",
            "[200 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}