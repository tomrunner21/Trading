{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st at HIBID single listing scrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvqonNtQ1vvpjN6hjAfnMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrunner21/Trading/blob/main/1st_at_HIBID_single_listing_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0hs9k6V1oW9"
      },
      "source": [
        "### started at 072921 at 5:37 am lol before PM ends:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IPuaN2N14by"
      },
      "source": [
        "# attrs=\"lot-tile-lead-container-header m-t-xs m-b-xs\"\n",
        "# lot-number-lead lot-link remove-underline lot-title-ellipsis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZO9XepbvXrr",
        "outputId": "76bab3f0-6231-4232-86dd-d09eac521be7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atP9U4S2vfVt",
        "outputId": "7156a2eb-3a42-48c2-8c53-6e53d7ef4bf0"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "titles = []\n",
        "\n",
        "def get_single_listing_header():\n",
        "\n",
        "  url = (\"https://hibid.com/catalog/298124/energy-corridor-estate-clearance/?g=all-categories&ipp=10\")\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  listing = html.find_all(attrs = {'class': 'lot-number-lead lot-link remove-underline lot-title-ellipsis'})\n",
        "  # listing = html.find(attrs = {'class': 'lot-tile-lead-container-header m-t-xs m-b-xs'})\n",
        "\n",
        "  print(listing)\n",
        "\n",
        "  for ticker in listing:\n",
        "    print(\"here\")\n",
        "    titles.append(ticker.get_text().strip())\n",
        "    print(ticker.get_text().strip())\n",
        "\n",
        "get_single_listing_header()\n",
        "# df = pd.DataFrame(list1)\n",
        "# print(df.size)\n",
        "# print(titles)\n",
        "# print(df)\n",
        "\n",
        "# df.to_csv('top_gainers_072721.csv')\n",
        "# !cp top_gainers_072721.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<a class=\"lot-number-lead lot-link remove-underline lot-title-ellipsis\" data-href=\"/lot/%7b0%7d/%7b1%7d/\" data-lot-label=\"Lot\" href=\"/lot/0/\">\n",
            "</a>]\n",
            "here\n",
            "\n",
            "['']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNCIubQKv9BH"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "# pages = np.arange(21, 121, 20)\n",
        "list1 = []\n",
        "\n",
        "def get_top_gainers2():\n",
        "  count = 0\n",
        "  url = (\"https://finviz.com/quote.ashx?t=bitf\")\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  news = html.find_all(attrs = {'class': 'fullview-news-outer'})\n",
        "  for ticker in news:\n",
        "    list1.append(ticker.get_text().strip())\n",
        "    test = ticker.get_text().strip()\n",
        "    lowercased = test.lower() # allows us to easily check for words that are not in start of sentence of news articles\n",
        "    print(lowercased)\n",
        "    if \"blockchain\" in lowercased:\n",
        "      print(\"True\")\n",
        "\n",
        "get_top_gainers2()\n",
        "\n",
        "# df.to_csv('top_gainers_072321.csv')\n",
        "# !cp top_gainers_072321.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}