{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finviz_topgainer_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3RIzSN79d5cOfgU4+SRhy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrunner21/Trading/blob/main/finviz_topgainer_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c21ohcPo1zTN",
        "outputId": "a531ed18-4c57-4743-93d5-a3475d1ff636"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnS9TaqAOkYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec90c7b-fcd3-4cfd-af85-5826cc63409c"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "pages = np.arange(21, 121, 20)\n",
        "list1 = []\n",
        "\n",
        "def get_top_gainers2():\n",
        "  count = 0\n",
        "  url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers\")\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  gainers1 = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "  for ticker in gainers1:\n",
        "    list1.append(ticker.get_text().strip())\n",
        "    # print(ticker.get_text().strip())\n",
        "\n",
        "  for page in pages:\n",
        "    # count += 1\n",
        "    # print(count)\n",
        "    url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    # print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    for ticker in gainers:\n",
        "      list1.append(ticker.get_text().strip())\n",
        "      # print(ticker.get_text().strip())\n",
        "\n",
        "get_top_gainers2()\n",
        "df = pd.DataFrame(list1)\n",
        "print(df.size)\n",
        "# print(list)\n",
        "# print(df)\n",
        "### why can this list somehow only hold 100 tickers????!\n",
        "\n",
        "# df.to_csv('top_gainers_072721.csv')\n",
        "# !cp top_gainers_072721.csv \"drive/My Drive/\"\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-GphjB8OnTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9ee250-f3e9-4e65-92e5-2028f20bb7ec"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "pages2 = np.arange(101, 121, 20)\n",
        "list2 = []\n",
        "\n",
        "def get_second_part():\n",
        "  for page in pages2:\n",
        "    # count += 1\n",
        "    # print(count)\n",
        "    url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    # print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    # print(gainers)\n",
        "    for ticker in gainers:\n",
        "      list2.append(ticker.get_text().strip())\n",
        "      # print(ticker.get_text().strip())\n",
        "\n",
        "get_second_part()\n",
        "df2 = pd.DataFrame(list2)\n",
        "print(df2.size)\n",
        "\n",
        "# df2.to_csv('top_gainers_072121_part2.csv')\n",
        "# !cp top_gainers_072121_part2.csv \"drive/My Drive/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ady3L8lFoz4J"
      },
      "source": [
        "result = df.append(df2)\n",
        "result.reset_index(drop=True, inplace=True)\n",
        "# result\n",
        "# result.to_csv('top_gainers_073021.csv')\n",
        "# !cp top_gainers_073021.csv \"drive/My Drive/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgkqMkgS65O"
      },
      "source": [
        "### below is the older editon of this notebook that kept messing up passed 100, the code above this is the newer part\n",
        "# I keep the bottom part just for reference but will probobly just delete as a final edition until automation or the current issue is solved. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZfuE9CxxZ8n"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "# from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "pages = np.arange(21, 201, 20)\n",
        "# pages = np.arange(21, 161, 20) #has to be 20 over the last page, so if last page is 141 put 161 so it includes 141\n",
        "# pages = np.arange(21, 121, 20) #6\n",
        "# pages = np.arange(21, 61, 20) #3\n",
        "# pages = np.arange(21, 101, 20) #5\n",
        "# pages = np.arange(101, 121, 20) #10\n",
        "######\n",
        "# when doing more than 5 pages, run part1 and part2 and combine in part3 to export:\n",
        "# pages = np.arange(21, 101, 20) \n",
        "pages2 = np.arange(101, 201, 20)\n",
        "list1 = []\n",
        "list2 = []\n",
        "######\n",
        "\n",
        "# list1 = []\n",
        "# def first_page():\n",
        "  \n",
        "# print('1')\n",
        "def get_top_gainers2():\n",
        "  count = 0\n",
        "  url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers\")\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  gainers1 = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "  for ticker in gainers1:\n",
        "    list1.append(ticker.get_text().strip())\n",
        "    # print(ticker.get_text().strip())\n",
        "\n",
        "  for page in pages:\n",
        "    # count += 1\n",
        "    # print(count)\n",
        "    url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    # print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    # print(gainers)\n",
        "    for ticker in gainers:\n",
        "      list1.append(ticker.get_text().strip())\n",
        "      # print(ticker.get_text().strip())\n",
        "      # print(ticker.get_text().strip())\n",
        "  # print(len(list))\n",
        "  # df = pd.DataFrame(list)\n",
        "\n",
        "def get_second_part():\n",
        "  for page in pages2:\n",
        "    # count += 1\n",
        "    # print(count)\n",
        "    url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    # print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    # print(gainers)\n",
        "    for ticker in gainers:\n",
        "      list2.append(ticker.get_text().strip())\n",
        "      # print(ticker.get_text().strip())\n",
        "\n",
        "\n",
        "get_top_gainers2()\n",
        "get_second_part()\n",
        "# print(list)\n",
        "# print(df)\n",
        "df = pd.DataFrame(list1)\n",
        "df2 = pd.DataFrame(list2)\n",
        "# print(list2)\n",
        "# print(len(list2))\n",
        "# print(df2)\n",
        "result = df.append(df2)\n",
        "# result\n",
        "# df2\n",
        "# df.tail(5)\n",
        "# for i in pages:\n",
        "#   print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(i))\n",
        "# df.to_csv('top_gainers_072121_180.csv')\n",
        "# !cp top_gainers_072121_180.csv \"drive/My Drive/\"\n",
        "\n",
        "# # more than 100:\n",
        "# df2.to_csv('top_gainers_072121_part3.csv')\n",
        "# !cp top_gainers_072121_part3.csv \"drive/My Drive/\"\n",
        "# print(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hbnZu22TPYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0q-pDYt6wRV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "pd.set_option('display.max_colwidth', 25)\n",
        "\n",
        "# Input\n",
        "# symbol = input('Enter a ticker: ')\n",
        "# print ('Getting data for ' + symbol + '...\\n')\n",
        "\n",
        "# Set up scraper\n",
        "# url = (\"http://finviz.com/quote.ashx?t=\" + symbol.lower() + \"&ty=c&p=d&b=1\")\n",
        "url = (\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers\")\n",
        "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "webpage = urlopen(req).read()\n",
        "html = soup(webpage, \"html.parser\")\n",
        "# print(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv2jFs4HFmA4",
        "outputId": "137964c5-4f0f-45b2-c0eb-40c0c6f7fc40"
      },
      "source": [
        "def get_top_gainers():\n",
        "  try:\n",
        "    page = pd.read_html(str(html))\n",
        "    # print(page)\n",
        "    # gainers = find_all()\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    for ticker in gainers:\n",
        "      print(ticker.get_text().strip())\n",
        "    return gainers \n",
        "\n",
        "  except Exception as e:\n",
        "        return e\n",
        "get_top_gainers()\n",
        "# gainers = pd.read_html(str(html), attrs = {'class': 'screener-link-primary'})\n",
        "      # https://finviz.com/screener.ashx?v=110&s=ta_topgainers\n",
        "      # https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=21\n",
        "      # https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=41\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CEMI\n",
            "NURO\n",
            "AEHR\n",
            "MSC\n",
            "NSPR\n",
            "SAVA\n",
            "LEXX\n",
            "MRIN\n",
            "XAIR\n",
            "DQ\n",
            "MMAT\n",
            "SNPX\n",
            "AMEH\n",
            "BBIG\n",
            "NMG\n",
            "FWP\n",
            "NM\n",
            "WISA\n",
            "BTU\n",
            "LE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<a class=\"screener-link-primary\" href=\"quote.ashx?t=CEMI&amp;ty=c&amp;p=d&amp;b=1\">CEMI</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=NURO&amp;ty=c&amp;p=d&amp;b=1\">NURO</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=AEHR&amp;ty=c&amp;p=d&amp;b=1\">AEHR</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=MSC&amp;ty=c&amp;p=d&amp;b=1\">MSC</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=NSPR&amp;ty=c&amp;p=d&amp;b=1\">NSPR</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=SAVA&amp;ty=c&amp;p=d&amp;b=1\">SAVA</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=LEXX&amp;ty=c&amp;p=d&amp;b=1\">LEXX</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=MRIN&amp;ty=c&amp;p=d&amp;b=1\">MRIN</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=XAIR&amp;ty=c&amp;p=d&amp;b=1\">XAIR</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=DQ&amp;ty=c&amp;p=d&amp;b=1\">DQ</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=MMAT&amp;ty=c&amp;p=d&amp;b=1\">MMAT</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=SNPX&amp;ty=c&amp;p=d&amp;b=1\">SNPX</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=AMEH&amp;ty=c&amp;p=d&amp;b=1\">AMEH</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=BBIG&amp;ty=c&amp;p=d&amp;b=1\">BBIG</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=NMG&amp;ty=c&amp;p=d&amp;b=1\">NMG</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=FWP&amp;ty=c&amp;p=d&amp;b=1\">FWP</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=NM&amp;ty=c&amp;p=d&amp;b=1\">NM</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=WISA&amp;ty=c&amp;p=d&amp;b=1\">WISA</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=BTU&amp;ty=c&amp;p=d&amp;b=1\">BTU</a>,\n",
              " <a class=\"screener-link-primary\" href=\"quote.ashx?t=LE&amp;ty=c&amp;p=d&amp;b=1\">LE</a>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYRfU3SPuPE8"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "pages = np.arange(21, 201, 20)\n",
        "# req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "headers={'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "for page in pages: \n",
        "  # req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  page = requests.get(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\" + str(page), headers=headers)\n",
        "  # print(page)\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "  \n",
        "  gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "  # for ticker in gainers:\n",
        "  #     print(ticker.get_text().strip())\n",
        "  # print(movie_div)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bbonp1MxALg",
        "outputId": "a796edc9-fdb6-4598-8c2c-a6a536a85035"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "pages = np.arange(21, 201, 20)\n",
        "\n",
        "for page in pages:\n",
        "  print(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=\"+  str(page) + \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=21\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=41\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=61\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=81\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=101\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=121\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=141\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=161\n",
            "\n",
            "https://finviz.com/screener.ashx?v=110&s=ta_topgainers&r=181\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}