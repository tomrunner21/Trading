{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated of webscrape single tickers news headliners.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR0rtCj2J8sIv71/H4gRar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrunner21/Trading/blob/main/Updated_of_webscrape_single_tickers_news_headliners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dwO19d1kULb",
        "outputId": "0b5ce6fb-a383-4134-c10e-1f1249ec92e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIaUdwrlBbuo",
        "outputId": "3dc15bc7-8df4-4f4f-c582-b83f26ffb4de"
      },
      "source": [
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "from random import randint\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "# pages = np.arange(21, 121, 20)\n",
        "list1 = []\n",
        "\n",
        "def get_top_gainers2():\n",
        "  count = 0\n",
        "  url = (\"https://finviz.com/quote.ashx?t=bitf\")\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "\n",
        "  news = html.find_all(attrs = {'class': 'fullview-news-outer'})\n",
        "  for ticker in news:\n",
        "    list1.append(ticker.get_text().strip())\n",
        "    test = ticker.get_text().strip()\n",
        "    lowercased = test.lower() # allows us to easily check for words that are not in start of sentence of news articles\n",
        "    print(lowercased)\n",
        "    if \"blockchain\" in lowercased:\n",
        "      print(\"True\")\n",
        "\n",
        "get_top_gainers2()\n",
        "\n",
        "# df.to_csv('top_gainers_072321.csv')\n",
        "# !cp top_gainers_072321.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jul-14-21 05:00pm  bitfarms to present at sequire blockchain event on july 15th globenewswire\n",
            "07:00am  bitfarms provides bitcoin production update globenewswire\n",
            "jun-30-21 07:00am  bitfarms announces grant of options and rsus globenewswire\n",
            "jun-23-21 01:40pm  bitfarms provides reminder of upcoming annual general and special meeting and issues addendum to previously filed management information circular globenewswire\n",
            "True\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7GeXA3OTLKj"
      },
      "source": [
        "#cleaned up not needed comments and changed the function's name that is related the purpose of this project -> 073121 @ 0630\n",
        "# still could run through a list of different words to check instead of a single word to find if it is in the headliners past\n",
        "# still need to store these headliners incase finviz deletes them later on (maybe)\n",
        "# still could instead of webscraping the enitre headeliners in bulk, but instead get each single line of the headliners so might me easier for data storage\n",
        "# still need to automate\n",
        "# still need to easily webscreape mulitple pages without np.arange and having to do it manually and change it manually daily "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}